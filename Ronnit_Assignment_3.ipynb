{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ronnit-Assignment-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiernee/Advanced_ML/blob/main/Week4/WOA7015_Wk4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5hkuTyx3VO"
      },
      "source": [
        "# Welcome to WOA7015 Advance Machine Learning Lab - Week 4\n",
        "This code is generated for the purpose of WOA7015 module.\n",
        "The code is available in github https://github.com/shiernee/Advanced_ML \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INxzH5Bqw5dq"
      },
      "source": [
        "## 1.0 Effect of weight and bias to sigmoid function\n",
        "This is the code to generate the figure in slide 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HctFBauH47o"
      },
      "source": [
        "#### 1.1 Effect of weight on sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah7KAg-fsGWY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# create sigmoid function\n",
        "f = lambda x, w, b: 1/(1 + np.exp(-(w*x + b)))\n",
        "\n",
        "x = np.arange(-10, 10, 0.01).reshape([-1, 1])\n",
        "\n",
        "# effect of weight on sigmoid function\n",
        "filenames = []\n",
        "for i in np.arange(1, 5, 0.1):\n",
        "  w = np.ones([1, 1]) * i * 0.5\n",
        "  b = np.ones([1, 1]) * 0\n",
        "\n",
        "  plt.plot(x, f(x, w, b))\n",
        "  plt.title('w = %0.1f' % i)\n",
        "  plt.grid()\n",
        "  plt.savefig('w %0.1f.png' % i)\n",
        "  plt.close()\n",
        "  filenames.append('w %0.1f.png' % i)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('w_mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYbm63A9H91J"
      },
      "source": [
        "#### 1.1 Effect of bias on sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1QreLFgHyIi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "# create sigmoid function\n",
        "f = lambda x, w, b: 1/(1 + np.exp(-(w*x + b)))\n",
        "\n",
        "x = np.arange(-10, 10, 0.01).reshape([-1, 1])\n",
        "\n",
        "# effect of bias on sigmoid function\n",
        "filenames = []\n",
        "for i in np.arange(1, 5, 0.1):\n",
        "  w = np.ones([1, 1])\n",
        "  b = np.ones([1, 1])* i\n",
        "\n",
        "  plt.plot(x, f(x, w, b))\n",
        "  plt.title('b = %0.1f' % i)\n",
        "  plt.grid()\n",
        "  plt.savefig('b %0.1f.png' % i)\n",
        "  plt.close()\n",
        "  filenames.append('b %0.1f.png' % i)\n",
        "\n",
        "# Build GIF\n",
        "with imageio.get_writer('b_mygif.gif', mode='I') as writer:\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXAZiEqSEbDH"
      },
      "source": [
        "# 2.0 Logistic Regression\n",
        "\n",
        "In this section, we will learn how to create train a Logistic Regression Model using pytorch. We will use MNIST image, as shown below. <br><br>\n",
        "\n",
        "PyTorch (https://pytorch.org/) is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. \n",
        "\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week4/MnistExamples.png\" width=\"512\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NXavLptEdGe"
      },
      "source": [
        "# 2.1 import library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKCbRxdFD86"
      },
      "source": [
        "#2.2 Set the Hyper-parameters \n",
        "input_size = 28 * 28  # 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXRWEdzFt_Z"
      },
      "source": [
        "#2.3 Data loader\n",
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True,  download = True,\n",
        "                                           transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, download = True,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "PCMGIa6ZLxWX",
        "outputId": "898f2d01-ad66-41da-dbec-423a03c16b4e"
      },
      "source": [
        "# 2.3.1 Check data \n",
        "print(train_dataset)\n",
        "print('----------------')\n",
        "print(test_dataset)\n",
        "print()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print('training data shape: ', train_dataset.data.shape)\n",
        "n = np.random.randint(0, 60000)\n",
        "plt.imshow(train_dataset.data[n])\n",
        "plt.title(f'n = %d label = %d' % (n, train_dataset.train_labels[n].numpy()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ../../data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "----------------\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ../../data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "\n",
            "training data shape:  torch.Size([60000, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'n = 35149 label = 1')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Loss - Cross Entropy Loss\n",
        "class custom_CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(custom_CrossEntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):      \n",
        "        num_examples = targets.shape[0]\n",
        "        batch_size = inputs.shape[0]\n",
        "        softmax_outputs = self.log_softmax(inputs)\n",
        "        outputs = softmax_outputs[range(batch_size), targets]        \n",
        "        return -torch.sum(outputs)/num_examples\n",
        "\n",
        "    @staticmethod\n",
        "    def log_softmax(x):\n",
        "      #return nn.LogSoftmax(x)\n",
        "      #import math\n",
        "      return torch.log(torch.exp(x) / torch.sum(torch.exp(x)) )"
      ],
      "metadata": {
        "id": "3V6vtAEdiKHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6ZE4roJF0HA"
      },
      "source": [
        "#2.4 Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXCxm64GH27K"
      },
      "source": [
        "#2.5 Cross Entropy Loss \n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm2Q-PDcH34b"
      },
      "source": [
        "#2.6 Optimizer Stochastic Gradient Descent \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TO7Egs7F2o9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632c7ec8-6d68-4e4a-90b5-e6383f541129"
      },
      "source": [
        "#2.7 Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 2.2545\n",
            "Epoch [1/5], Step [200/600], Loss: 2.0974\n",
            "Epoch [1/5], Step [300/600], Loss: 2.0234\n",
            "Epoch [1/5], Step [400/600], Loss: 1.9766\n",
            "Epoch [1/5], Step [500/600], Loss: 1.8297\n",
            "Epoch [1/5], Step [600/600], Loss: 1.8086\n",
            "Epoch [2/5], Step [100/600], Loss: 1.7204\n",
            "Epoch [2/5], Step [200/600], Loss: 1.6799\n",
            "Epoch [2/5], Step [300/600], Loss: 1.6386\n",
            "Epoch [2/5], Step [400/600], Loss: 1.5646\n",
            "Epoch [2/5], Step [500/600], Loss: 1.5540\n",
            "Epoch [2/5], Step [600/600], Loss: 1.4932\n",
            "Epoch [3/5], Step [100/600], Loss: 1.4126\n",
            "Epoch [3/5], Step [200/600], Loss: 1.4065\n",
            "Epoch [3/5], Step [300/600], Loss: 1.3075\n",
            "Epoch [3/5], Step [400/600], Loss: 1.2869\n",
            "Epoch [3/5], Step [500/600], Loss: 1.3336\n",
            "Epoch [3/5], Step [600/600], Loss: 1.3469\n",
            "Epoch [4/5], Step [100/600], Loss: 1.2589\n",
            "Epoch [4/5], Step [200/600], Loss: 1.2251\n",
            "Epoch [4/5], Step [300/600], Loss: 1.1899\n",
            "Epoch [4/5], Step [400/600], Loss: 1.1497\n",
            "Epoch [4/5], Step [500/600], Loss: 1.0838\n",
            "Epoch [4/5], Step [600/600], Loss: 1.0862\n",
            "Epoch [5/5], Step [100/600], Loss: 1.1475\n",
            "Epoch [5/5], Step [200/600], Loss: 1.0713\n",
            "Epoch [5/5], Step [300/600], Loss: 1.1856\n",
            "Epoch [5/5], Step [400/600], Loss: 1.0701\n",
            "Epoch [5/5], Step [500/600], Loss: 1.0778\n",
            "Epoch [5/5], Step [600/600], Loss: 1.0923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGO_mdMZF4iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0341b1-ad93-473e-b326-a33cb6c93ddc"
      },
      "source": [
        "#2.8 Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the 10000 test images: 82.5999984741211 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBEBeGYsF5t-"
      },
      "source": [
        "#2.9 Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6nIkwr2YM9l"
      },
      "source": [
        "## Exercise 1 (10%): Create custom loss function\n",
        "In this section, you will need to create our own Cross Entropy loss function and compare with Pytorch's Cross Entropy loss. The objective of this exercise is to enable you to design your own loss in the future. \n",
        "\n",
        "Follow the steps below:\n",
        "1. Import libraries - copy section 2.1\n",
        "2. Set hyperparameter - copy section 2.2\n",
        "3. Data loader - copy section 2.3\n",
        "4. Initialize Logistic Regression - copy section 2.4\n",
        "5. Create custom_CrossEntropyLoss class - copy the following code. Your task is to ***code the log_softmax equation in the log_softmax function.*** \n",
        "\n",
        "```\n",
        "#  Custom Loss - Cross Entropy Loss\n",
        "class custom_CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(custom_CrossEntropyLoss, self).__init__()\n",
        " \n",
        "    def forward(self, inputs, targets, smooth=1):      \n",
        "        num_examples = targets.shape[0]\n",
        "        batch_size = inputs.shape[0]\n",
        "        softmax_outputs = self.log_softmax(inputs)\n",
        "        outputs = softmax_outputs[range(batch_size), targets]        \n",
        "        return -torch.sum(outputs)/num_examples\n",
        "\n",
        "    @staticmethod\n",
        "    def log_softmax(x):\n",
        "      return ### put the log_softmax function here ### \n",
        "```\n",
        "\n",
        "6. Initialize custom_CrossEntropyLoss loss as criterion - copy section 2.5. Replace *nn.CrossEntropyLoss* with *custom_CrossEntropyLoss*\n",
        "7. Train the model, evaluate it on your testing data. Save your model. \n",
        "8. Compare the loss computed from torch and our custom loss.  \n",
        "\n",
        "\n",
        " 5% will be  given if step 1 - 4 are done correctly <br>\n",
        " 3% will be  given if step 5-7 is done correctly <br>\n",
        " 2% will be given if your custom loss and pytorch loss is near zero. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7azur5ciavP2"
      },
      "source": [
        "# your code here\n",
        "# 2.1 import library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.2 Set the Hyper-parameters \n",
        "input_size = 28 * 28  # 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "ACEgbPCqGab1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.3 Data loader\n",
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True,  download = True,\n",
        "                                           transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, download = True,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n"
      ],
      "metadata": {
        "id": "owCVh_wYGe9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.4 Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n"
      ],
      "metadata": {
        "id": "Aq8x2YQJGfCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Loss - Cross Entropy Loss\n",
        "class custom_CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(custom_CrossEntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):      \n",
        "        num_examples = targets.shape[0]\n",
        "        batch_size = inputs.shape[0]\n",
        "        softmax_outputs = self.log_softmax(inputs)\n",
        "        outputs = softmax_outputs[range(batch_size), targets]        \n",
        "        return -torch.sum(outputs)/num_examples\n",
        "\n",
        "    @staticmethod\n",
        "    def log_softmax(x):\n",
        "      return torch.log(torch.exp(x) / torch.sum(torch.exp(x)))"
      ],
      "metadata": {
        "id": "JshnZ1fHihyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.5 Cross Entropy Loss \n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = custom_CrossEntropyLoss(nn.Module)"
      ],
      "metadata": {
        "id": "qjKtvEdRip4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.6 Optimizer Stochastic Gradient Descent \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
      ],
      "metadata": {
        "id": "R3S8nO67ityP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.7 Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, input_size)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nILzp0zyizFo",
        "outputId": "969d54ed-81c7-45e0-d533-bf34a98c593d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 5.3804\n",
            "Epoch [1/5], Step [200/600], Loss: 5.2644\n",
            "Epoch [1/5], Step [300/600], Loss: 5.2176\n",
            "Epoch [1/5], Step [400/600], Loss: 5.1986\n",
            "Epoch [1/5], Step [500/600], Loss: 5.1576\n",
            "Epoch [1/5], Step [600/600], Loss: 5.2389\n",
            "Epoch [2/5], Step [100/600], Loss: 5.3093\n",
            "Epoch [2/5], Step [200/600], Loss: 5.2928\n",
            "Epoch [2/5], Step [300/600], Loss: 5.1947\n",
            "Epoch [2/5], Step [400/600], Loss: 5.1226\n",
            "Epoch [2/5], Step [500/600], Loss: 5.2559\n",
            "Epoch [2/5], Step [600/600], Loss: 5.2872\n",
            "Epoch [3/5], Step [100/600], Loss: 5.1833\n",
            "Epoch [3/5], Step [200/600], Loss: 5.1890\n",
            "Epoch [3/5], Step [300/600], Loss: 5.1888\n",
            "Epoch [3/5], Step [400/600], Loss: 5.1083\n",
            "Epoch [3/5], Step [500/600], Loss: 5.2368\n",
            "Epoch [3/5], Step [600/600], Loss: 5.2099\n",
            "Epoch [4/5], Step [100/600], Loss: 5.2004\n",
            "Epoch [4/5], Step [200/600], Loss: 5.3095\n",
            "Epoch [4/5], Step [300/600], Loss: 5.2013\n",
            "Epoch [4/5], Step [400/600], Loss: 5.1610\n",
            "Epoch [4/5], Step [500/600], Loss: 5.2963\n",
            "Epoch [4/5], Step [600/600], Loss: 5.0673\n",
            "Epoch [5/5], Step [100/600], Loss: 5.1432\n",
            "Epoch [5/5], Step [200/600], Loss: 5.3291\n",
            "Epoch [5/5], Step [300/600], Loss: 5.1603\n",
            "Epoch [5/5], Step [400/600], Loss: 5.1384\n",
            "Epoch [5/5], Step [500/600], Loss: 5.1163\n",
            "Epoch [5/5], Step [600/600], Loss: 5.2102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.8 Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, input_size)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38SAvnQhi31n",
        "outputId": "564b6487-a1bd-45ba-b877-1a453bc9a400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the 10000 test images: 90.91000366210938 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.9 Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model_custom.ckpt')"
      ],
      "metadata": {
        "id": "ruQ_U_J9i8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7IdSoak0p2"
      },
      "source": [
        "# Submission Instructions\n",
        "Once you are finished, follow these steps:\n",
        "\n",
        "Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells.\n",
        "If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n",
        "Double check that there is a number next to each code cell and that these numbers are in order.\n",
        "Then, submit your lab as follows:\n",
        "\n",
        "Go to File > Print > Save as PDF.\n",
        "Double check that the entire notebook, from beginning to end, is in this PDF file. Make sure Solution for Exercise 5 are in for marks. \n",
        "Upload the PDF to Spectrum. "
      ]
    }
  ]
}